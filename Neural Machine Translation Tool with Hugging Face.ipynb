{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3a1928e-a0de-4fc6-9c1e-03905a31ef29",
   "metadata": {},
   "source": [
    "# üåç Neural Machine Translation Tool with Hugging Face\n",
    "\n",
    "## üìå Project Overview\n",
    "\n",
    "This project implements a **Neural Machine Translation (NMT)** system using\n",
    "pretrained transformer models from **Hugging Face**.  \n",
    "The application translates text between languages using the MarianMT architecture.\n",
    "\n",
    "The goal of this project is to demonstrate:\n",
    "- Practical use of pretrained NLP models\n",
    "- Text preprocessing and tokenization\n",
    "- Model inference and decoding\n",
    "- Real-world AI application design\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bf8af5-dec2-42b9-984f-d67db46abc01",
   "metadata": {},
   "source": [
    "## üåç Real-World Use Case\n",
    "\n",
    "Language translation systems are widely used in:\n",
    "- Customer support chatbots\n",
    "- International e-commerce platforms\n",
    "- Content localization\n",
    "- Educational and accessibility tools\n",
    "\n",
    "This project simulates how a production-ready translation service\n",
    "could be integrated into web or mobile applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55bc55d-ab24-4b76-b176-567d20943d5f",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "* `transformers`: For pre-trained models.\n",
    "* `sentencepiece` & `sacremoses`: For tokenization and text processing.\n",
    "* `torch`: PyTorch backend for deep learning operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9069daf9-3c17-441a-9a91-a7821c71e7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.57.3)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.2.1)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: torch in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2025.10.23)\n",
      "Requirement already satisfied: requests in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: click in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sacremoses) (8.3.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sacremoses) (1.5.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "# 'transformers' provides the model architecture\n",
    "# 'sentencepiece' and 'sacremoses' are required for MarianMT tokenization\n",
    "!pip install transformers sentencepiece sacremoses torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfdd01a-11d2-4fb1-8ce1-1b4dc986e054",
   "metadata": {},
   "source": [
    "## Imports and Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fabb2d3b-c4bd-4e47-b2a4-add2e419b3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: CPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from typing import List, Union\n",
    "\n",
    "# Set up device: Use GPU (CUDA for NVIDIA, MPS for Mac M-chips) if available, otherwise CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device.upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aece5d98-56de-473d-b331-8e205f2d8388",
   "metadata": {},
   "source": [
    "## The Translator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b72a0a3d-ff7d-4376-b329-4f7db25746cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageTranslator:\n",
    "    def __init__(self, source_lang: str, target_lang: str):\n",
    "        \"\"\"\n",
    "        Initializes the translator by loading the specific MarianMT model.\n",
    "        \n",
    "        Args:\n",
    "            source_lang (str): Source language code (e.g., 'en').\n",
    "            target_lang (str): Target language code (e.g., 'fr', 'ar', 'es').\n",
    "        \"\"\"\n",
    "        self.model_name = f'Helsinki-NLP/opus-mt-{source_lang}-{target_lang}'\n",
    "        print(f\"‚è≥ Loading model: {self.model_name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Load tokenizer and model\n",
    "            self.tokenizer = MarianTokenizer.from_pretrained(self.model_name)\n",
    "            self.model = MarianMTModel.from_pretrained(self.model_name).to(device)\n",
    "            print(f\"‚úÖ Model loaded successfully on {device.upper()}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading model: {e}\")\n",
    "            print(\"Check if the language pair exists in Hugging Face Hub.\")\n",
    "\n",
    "    def translate(self, text: Union[str, List[str]], **kwargs) -> Union[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Translates text or a list of texts.\n",
    "        \n",
    "        Args:\n",
    "            text (str or List[str]): Input text(s) to translate.\n",
    "            **kwargs: Additional arguments for model.generate() (e.g., num_beams).\n",
    "            \n",
    "        Returns:\n",
    "            str or List[str]: Translated text(s).\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "\n",
    "        # Prepare input batch\n",
    "        inputs = self.tokenizer(\n",
    "            text, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True, \n",
    "            truncation=True\n",
    "        ).to(device)\n",
    "\n",
    "        # Generate translation using model parameters\n",
    "        # num_beams=4 gives better quality than greedy search\n",
    "        translated_ids = self.model.generate(\n",
    "            **inputs, \n",
    "            max_length=200, \n",
    "            num_beams=kwargs.get('num_beams', 4),\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "        # Decode generated IDs back to text\n",
    "        translated_text = self.tokenizer.batch_decode(translated_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Return string if input was string, else list\n",
    "        return translated_text[0] if isinstance(text, str) else translated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e889d0-7d5f-4646-aaa9-fe94a5136703",
   "metadata": {},
   "source": [
    "## üöÄ Usage Examples\n",
    "\n",
    "### 1. English to Arabic Translation\n",
    "We will initialize the translator for `en` (English) to `ar` (Arabic) and translate a sample sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c27cd09e-e568-446e-b3ee-ab588522cafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading model: Helsinki-NLP/opus-mt-en-ar...\n",
      "‚úÖ Model loaded successfully on CPU.\n",
      "\n",
      "Original: Artificial Intelligence is transforming the world.\n",
      "Arabic: ÿßŸÑÿ•ÿ≥ÿ™ÿÆÿ®ÿßÿ±ÿßÿ™ ÿßŸÑÿ•ÿµÿ∑ŸÜÿßÿπŸäÿ© ÿ™ÿ≠ŸàŸÑ ÿßŸÑÿπÿßŸÑŸÖ\n"
     ]
    }
   ],
   "source": [
    "# Initialize translator for English to Arabic\n",
    "en_to_ar = LanguageTranslator(source_lang=\"en\", target_lang=\"ar\")\n",
    "\n",
    "# Single sentence translation\n",
    "text_ar = \"Artificial Intelligence is transforming the world.\"\n",
    "translation_ar = en_to_ar.translate(text_ar)\n",
    "\n",
    "print(f\"\\nOriginal: {text_ar}\")\n",
    "print(f\"Arabic: {translation_ar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1877b363-d0eb-4894-a069-1c96b4d25252",
   "metadata": {},
   "source": [
    "### 2. Batch Translation (English to French)\n",
    "Processing a list of sentences simultaneously is much faster than a `for` loop because the model can parallelize the operation on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb30cb41-18c1-4704-9d0f-1b6b41b3f709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading model: Helsinki-NLP/opus-mt-en-fr...\n",
      "‚úÖ Model loaded successfully on CPU.\n",
      "\n",
      "--- Batch Translation Results ---\n",
      "üá∫üá∏: Hello, how are you? \n",
      "üá´üá∑: Bonjour, comment √ßa va ?\n",
      "\n",
      "üá∫üá∏: The weather is beautiful today. \n",
      "üá´üá∑: Le temps est beau aujourd'hui.\n",
      "\n",
      "üá∫üá∏: Machine learning models are fascinating. \n",
      "üá´üá∑: Les mod√®les d'apprentissage automatique sont fascinants.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize translator for English to French\n",
    "en_to_fr = LanguageTranslator(source_lang=\"en\", target_lang=\"fr\")\n",
    "\n",
    "# Batch of sentences\n",
    "batch_texts = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"The weather is beautiful today.\",\n",
    "    \"Machine learning models are fascinating.\"\n",
    "]\n",
    "\n",
    "# Translate batch\n",
    "translations_fr = en_to_fr.translate(batch_texts)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n--- Batch Translation Results ---\")\n",
    "for original, translated in zip(batch_texts, translations_fr):\n",
    "    print(f\"üá∫üá∏: {original} \\nüá´üá∑: {translated}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234942b2-6d92-4717-9189-3a122deecf86",
   "metadata": {},
   "source": [
    "## üß† Technical Note: Generation Parameters\n",
    "\n",
    "When calling `.translate()`, we use `model.generate()`. Here is what the parameters mean:\n",
    "\n",
    "* **`num_beams`**: Enables **Beam Search**. Instead of picking the single best next word (Greedy Search), it keeps track of the top `n` most likely sequences. Higher beams = better quality but slower.\n",
    "* **`early_stopping`**: Stops generation when all beam hypotheses reach the end-of-sentence token.\n",
    "* **`max_length`**: Limits the number of generated tokens to prevent infinite loops or excessively long outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09da12c8-99b9-4d5c-82aa-f67ee5a8a10a",
   "metadata": {},
   "source": [
    "## üèÅ Conclusion & Future Scope\n",
    "\n",
    "In this notebook, we successfully built a modular **Neural Machine Translation (NMT)** tool capable of translating between multiple languages using the **MarianMT** architecture. By leveraging Hugging Face Transformers, we achieved high-quality translations without the need for training a model from scratch.\n",
    "\n",
    "\n",
    "\n",
    "### **Key Achievements**\n",
    "* **Abstraction:** Encapsulated complex logic into a reusable `LanguageTranslator` class.\n",
    "* **Performance:** Implemented automatic hardware acceleration (GPU detection) and batch processing for high-throughput inference.\n",
    "* **Flexibility:** The system supports dynamic switching between hundreds of language pairs available in the OPUS-MT collection.\n",
    "\n",
    "### **üîÆ Future Improvements**\n",
    "To elevate this project from a script to a production-grade application, the following enhancements are recommended:\n",
    "\n",
    "1.  **User Interface (GUI):** Wrap the class in a **Streamlit** or **Gradio** application to provide a web interface for non-technical users.\n",
    "2.  **Model Quantization:** Convert the model to **ONNX** format or use 8-bit quantization to drastically reduce memory usage and improve latency on CPUs.\n",
    "3.  **API Deployment:** Serve the model via a **FastAPI** endpoint to allow other software services to request translations programmatically.\n",
    "4.  **Domain Adaptation:** Fine-tune the model on specific datasets (e.g., medical or legal documents) to improve accuracy for specialized terminology."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
